{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4269ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import sklearn.metrics as metrics\n",
    "from yolo import YOLO\n",
    "import time\n",
    "from cvzone.HandTrackingModule import HandDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8e50962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    image = cv2.resize(image,(224,224))\n",
    "    image = image.astype(\"float32\")\n",
    "    image = image.reshape(1,224,224,3)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f27c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(weights=None,\n",
    "                include_top=False,\n",
    "                input_tensor=Input(shape=(224, 224, 3)))\n",
    "def createModel1():\n",
    "    global model\n",
    "    h   = model.output\n",
    "    h   = AveragePooling2D(pool_size=(5, 5))(h)\n",
    "    h   = Flatten(name=\"flatten\")(h)\n",
    "    #h   = Dense(512, activation=\"relu\")(h)\n",
    "    h   = Dense(128, activation=\"relu\")(h)\n",
    "    h   = Dropout(0.5)(h)\n",
    "    h   = Dense(9, activation=\"softmax\")(h)\n",
    "    model = Model(inputs=model.input, outputs=h)\n",
    "    return model\n",
    "model = createModel1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db69f02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"modelsensing1.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bd4bafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gesture_detect(frame, model, yolo):\n",
    "    width, height, inference_time, results = yolo.inference(frame)\n",
    "    # display fps\n",
    "    cv2.putText(frame, f'{round(1/inference_time,2)} FPS', (15,15), cv2.FONT_HERSHEY_SIMPLEX,0.5, (0,255,255), 2)\n",
    "\n",
    "    # sort by confidence\n",
    "    results.sort(key=lambda x: x[2])\n",
    "\n",
    "    # how many hands should be shown\n",
    "    hand_count = len(results)\n",
    "    #if args.hands != -1:\n",
    "    hand_count = int(1)\n",
    "    # display hands\n",
    "    box = tuple()\n",
    "    ok = 0\n",
    "    for detection in results[:hand_count]:\n",
    "        id, name, confidence, x, y, w, h = detection\n",
    "        cx = x + (w / 2)\n",
    "        cy = y + (h / 2)\n",
    "        roi_color = frame[y:y+h, x:x+w, :]\n",
    "        roi_color = preprocess(roi_color)\n",
    "        predicts_m = model(roi_color,training = False)\n",
    "        predicts = np.array(predicts_m)\n",
    "        maxindex1 = int(np.argmax(predicts))\n",
    "        # draw a bounding box rectangle and label on the image\n",
    "        color = (0, 255, 255)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "        text = \"%s (%s)\" % (name, round(confidence, 2))\n",
    "        cv2.putText(frame, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5, color, 2)\n",
    "        cv2.putText(frame, gestrue_dict[maxindex1], (x, y + 15), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5, color, 2)\n",
    "        box =  tuple([x, y, w, h])\n",
    "        ok = 1\n",
    "    return frame,box,ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "258acc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class general_pose_model(object):\n",
    "    def __init__(self, modelpath):\n",
    "        self.num_points = 22\n",
    "        self.point_pairs = [[0,1],[1,2],[2,3],[3,4],\n",
    "                            [0,5],[5,6],[6,7],[7,8],\n",
    "                            [0,9],[9,10],[10,11],[11,12],\n",
    "                            [0,13],[13,14],[14,15],[15,16],\n",
    "                            [0,17],[17,18],[18,19],[19,20]]\n",
    "        # self.inWidth = 368\n",
    "        self.inHeight = 360\n",
    "        self.threshold = 0.1\n",
    "        self.hand_net = self.get_hand_model(modelpath)\n",
    "\n",
    "\n",
    "    def get_hand_model(self, modelpath):\n",
    "\n",
    "        prototxt   = os.path.join(modelpath, \"./pose_deploy.prototxt\")\n",
    "        caffemodel = os.path.join(modelpath, \"./pose_iter_102000.caffemodel\")\n",
    "        hand_model = cv2.dnn.readNetFromCaffe(prototxt, caffemodel)\n",
    "\n",
    "        return hand_model\n",
    "\n",
    "\n",
    "    def predict(self, imgfile):\n",
    "        #img_cv2 = cv2.imread(imgfile)\n",
    "        img_cv2 = imgfile\n",
    "        [img_height, img_width, _ ]= img_cv2.shape\n",
    "        aspect_ratio = img_width / img_height\n",
    "\n",
    "        inWidth = int(((aspect_ratio * self.inHeight) * 8) // 8)\n",
    "        inpBlob = cv2.dnn.blobFromImage(img_cv2, 1.0 / 255, (inWidth, self.inHeight), (0, 0, 0), swapRB=False, crop=False)\n",
    "\n",
    "        self.hand_net.setInput(inpBlob)\n",
    "        output = self.hand_net.forward()\n",
    "        points = []\n",
    "        for idx in range(self.num_points):\n",
    "            probMap = output[0, idx, :, :] # confidence map.\n",
    "            probMap = cv2.resize(probMap, (img_width, img_height))\n",
    "            # Find global maxima of the probMap.\n",
    "            minVal, prob, minLoc, point = cv2.minMaxLoc(probMap)\n",
    "\n",
    "            if prob > self.threshold:\n",
    "                points.append((int(point[0]), int(point[1])))\n",
    "            else:\n",
    "                points.append(None)\n",
    "\n",
    "        return points\n",
    "\n",
    "    def vis_pose(self, imgfile, points):\n",
    "        img_cv2 = imgfile\n",
    "        # Draw Skeleton\n",
    "        for pair in self.point_pairs:\n",
    "            partA = pair[0]\n",
    "            partB = pair[1]\n",
    "            if points[partA] and points[partB]:\n",
    "                cv2.line(img_cv2, points[partA], points[partB], (0, 255, 255), 3)\n",
    "                cv2.circle(img_cv2, points[partA], 8, (0, 0, 255), thickness=-1, lineType=cv2.FILLED)\n",
    "\n",
    "        return img_cv2\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0a8473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ellipse_detect(image):\n",
    "    \"\"\"YCrCb颜色空间的Cr分量+Otsu阈值分割\n",
    "    :param image: 图片路径\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    img = image\n",
    "    ycrcb = cv2.cvtColor(img, cv2.COLOR_BGR2YCR_CB)\n",
    " \n",
    "    (y, cr, cb) = cv2.split(ycrcb)\n",
    "    cr1 = cv2.GaussianBlur(cr, (5, 5), 0)\n",
    "    _, skin = cv2.threshold(cr1,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    skin = cv2.cvtColor(skin,cv2.COLOR_GRAY2BGR)\n",
    "    return skin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53daabc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting webcam...\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#if args.network == \"normal\":\n",
    "#    print(\"loading yolo...\")\n",
    "#    yolo = YOLO(\"models/cross-hands.cfg\", \"models/cross-hands.weights\", [\"hand\"])\n",
    "#elif args.network == \"prn\":\n",
    "#    print(\"loading yolo-tiny-prn...\")\n",
    "#    yolo = YOLO(\"models/cross-hands-tiny-prn.cfg\", \"models/cross-hands-tiny-prn.weights\", [\"hand\"])\n",
    "#elif args.network == \"v4-tiny\":\n",
    "#    print(\"loading yolov4-tiny-prn...\")\n",
    "#    yolo = YOLO(\"models/cross-hands-yolov4-tiny.cfg\", \"models/cross-hands-yolov4-tiny.weights\", [\"hand\"])\n",
    "#else:\n",
    "#    print(\"loading yolo-tiny...\")\n",
    "#    yolo = YOLO(\"models/cross-hands-tiny.cfg\", \"models/cross-hands-tiny.weights\", [\"hand\"])\n",
    "yolo = YOLO(\"models/cross-hands-tiny.cfg\", \"models/cross-hands-tiny.weights\", [\"hand\"])\n",
    "yolo.size = int(416)\n",
    "yolo.confidence = float(0.2)\n",
    "#pose model init\n",
    "modelpath = \"./models\"\n",
    "\n",
    "#gestrue_dict\n",
    "gestrue_dict = {0: \"fist\", 1: \"five\", 2: \"gundown\", 3: \"gunup\", 4: \"one\", 5: \"thumbdown\", 6: \"thumbleft\", 7: \"thumbright\", 8: \"thumbup\"}\n",
    "print(\"starting webcam...\")\n",
    "cv2.namedWindow(\"preview\")\n",
    "vc = cv2.VideoCapture(0)\n",
    "WW = int(vc.get(3))\n",
    "HH = int(vc.get(4))\n",
    "\n",
    "detector = HandDetector(maxHands=2,detectionCon=0.8)\n",
    "#tracker init\n",
    "tracker = cv2.TrackerKCF_create()\n",
    "\n",
    "if vc.isOpened():  # try to get the first frame\n",
    "    rval, frame = vc.read()\n",
    "else:\n",
    "    rval = False\n",
    "\n",
    "op_mode = 1 #0: gestrue 1: 21 points\n",
    "hand_track = 0\n",
    "while rval:\n",
    "    if(op_mode == 0):\n",
    "        if(hand_track == 0):\n",
    "            skin = ellipse_detect(frame)\n",
    "            print(\"1\")\n",
    "            frame,bbox,ok = gesture_detect(frame, model, yolo)\n",
    "            print(\"2\")\n",
    "            if(ok == 1):\n",
    "                #print(\"1\")\n",
    "                tracker = cv2.TrackerKCF_create()\n",
    "                ok = tracker.init(skin, bbox)\n",
    "                #print(\"2\")\n",
    "                hand_track = 1\n",
    "            #cv2.imshow(\"preview\", frame)\n",
    "            #cv2.waitKey(0)\n",
    "        else:\n",
    "            start_time = time.time()\n",
    "            #skin = ellipse_detect(frame)\n",
    "            #print(\"1\")\n",
    "            #ok, tbox = tracker.update(skin)\n",
    "            #print(\"2\")\n",
    "            ok = 1\n",
    "            if ok:\n",
    "                #p1 = (max(int(tbox[0] - 20),0), max(int(tbox[1] - 20),0))\n",
    "                #p2 = (min(int(tbox[0] + tbox[2]+20), WW), min(int(tbox[1] + tbox[3]+20), HH))\n",
    "                #print(p1, p2, WW, HH)\n",
    "                #cv2.rectangle(frame, p1, p2, (255,0,0), 2, 1)\n",
    "                #roi_color = frame[p1[1]:p2[1], p1[0]:p2[0], :]\n",
    "                #roi_color = preprocess(roi_color)\n",
    "                roi_color = preprocess(frame)\n",
    "                predicts_m = model(roi_color,training = False)\n",
    "                predicts = np.array(predicts_m)\n",
    "                maxindex1 = int(np.argmax(predicts))\n",
    "                #cv2.putText(frame, gestrue_dict[maxindex1], (p1[0], p2[1] + 15), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    #0.5, (0, 255, 255), 2)\n",
    "            else:\n",
    "                hand_track = 0\n",
    "            fpp = 1 / (time.time() - start_time)\n",
    "            cv2.putText(frame, \"Tracker, Frame \" + str(fpp), (10,20), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (50,170,50),2)\n",
    "    elif(op_mode == 1):\n",
    "        start_time = time.time()\n",
    "        hand, frame = detector.findHands(frame)\n",
    "        fpp = 1 / (time.time() - start_time)\n",
    "        cv2.putText(frame, \"Tracker, Frame \" + str(fpp), (10,20), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (50,170,50),2)\n",
    "        \n",
    "    cv2.imshow(\"preview\", frame)\n",
    "    rval, frame = vc.read()\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord(\"q\"):  # exit on ESC\n",
    "        if(op_mode == 1):\n",
    "            op_mode = 0\n",
    "        elif(op_mode == 0):\n",
    "            op_mode = 1\n",
    "    if key == 27:  # exit on ESC\n",
    "        break\n",
    "\n",
    "cv2.destroyWindow(\"preview\")\n",
    "vc.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c90fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
